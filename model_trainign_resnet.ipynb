{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/Facial Analysis/archive.zip'\n",
        "local_dest = '/content/'\n",
        "\n",
        "print(f\"Copying {zip_path} to {local_dest}...\")\n",
        "!cp \"{zip_path}\" \"{local_dest}\"\n",
        "\n",
        "print(\"Extracting archive...\")\n",
        "with zipfile.ZipFile(os.path.join(local_dest, 'archive.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(local_dest)\n",
        "\n",
        "TRAIN_PATH = '/content/train'\n",
        "TEST_PATH = '/content/test'\n",
        "\n",
        "print(f\"\\nDataset extracted!\")\n",
        "print(f\"Train path: {TRAIN_PATH}\")\n",
        "print(f\"Test path: {TEST_PATH}\")\n",
        "\n",
        "import os\n",
        "if os.path.exists(TRAIN_PATH):\n",
        "    train_classes = os.listdir(TRAIN_PATH)\n",
        "    print(f\"\\nTrain classes found: {train_classes}\")\n",
        "    for cls in train_classes:\n",
        "        cls_path = os.path.join(TRAIN_PATH, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            print(f\"  {cls}: {len(os.listdir(cls_path))} images\")\n",
        "\n",
        "if os.path.exists(TEST_PATH):\n",
        "    test_classes = os.listdir(TEST_PATH)\n",
        "    print(f\"\\nTest classes found: {test_classes}\")\n",
        "    for cls in test_classes:\n",
        "        cls_path = os.path.join(TEST_PATH, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            print(f\"  {cls}: {len(os.listdir(cls_path))} images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import math\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Conv2dSame(torch.nn.Conv2d):\n",
        "    def calc_same_pad(self, i: int, k: int, s: int, d: int) -> int:\n",
        "        return max((math.ceil(i / s) - 1) * s + (k - 1) * d + 1 - i, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        ih, iw = x.size()[-2:]\n",
        "        pad_h = self.calc_same_pad(i=ih, k=self.kernel_size[0], s=self.stride[0], d=self.dilation[0])\n",
        "        pad_w = self.calc_same_pad(i=iw, k=self.kernel_size[1], s=self.stride[1], d=self.dilation[1])\n",
        "\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n",
        "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "print(\"Conv2dSame class defined - Matches realtime_facial_analysis.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.99)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding='same', bias=False)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.99)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion, eps=0.001, momentum=0.99)\n",
        "        \n",
        "        self.i_downsample = i_downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
        "        x = self.conv3(x)\n",
        "        x = self.batch_norm3(x)\n",
        "        \n",
        "        if self.i_downsample is not None:\n",
        "            identity = self.i_downsample(identity)\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "print(\"Bottleneck class defined - expansion=4, BatchNorm(eps=0.001, momentum=0.99)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv_layer_s2_same = Conv2dSame(num_channels, 64, 7, stride=2, groups=1, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64, eps=0.001, momentum=0.99)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        \n",
        "        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64, stride=1)\n",
        "        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n",
        "        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=256, stride=2)\n",
        "        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=512, stride=2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc1 = nn.Linear(512*ResBlock.expansion, 512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        x = self.relu(self.batch_norm1(self.conv_layer_s2_same(x)))\n",
        "        x = self.max_pool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.extract_features(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "        \n",
        "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
        "        ii_downsample = None\n",
        "        layers = []\n",
        "        \n",
        "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
        "            ii_downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride, bias=False, padding=0),\n",
        "                nn.BatchNorm2d(planes*ResBlock.expansion, eps=0.001, momentum=0.99)\n",
        "            )\n",
        "            \n",
        "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
        "        self.in_channels = planes*ResBlock.expansion\n",
        "        \n",
        "        for i in range(blocks-1):\n",
        "            layers.append(ResBlock(self.in_channels, planes))\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "def ResNet50(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes, channels)\n",
        "\n",
        "print(\"ResNet class defined with exact architecture:\")\n",
        "print(\"  - Conv2dSame 7x7 stride=2 -> BatchNorm -> ReLU -> MaxPool 3x3 stride=2\")\n",
        "print(\"  - Layer blocks: [3,4,6,3] with planes [64,128,256,512]\")\n",
        "print(\"  - FC layers: 2048 -> 512 -> 7 classes\")\n",
        "print(\"  - BatchNorm everywhere: eps=0.001, momentum=0.99\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FER2013Dataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, is_training=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.is_training = is_training\n",
        "        \n",
        "        self.emotion_map = {\n",
        "            'angry': 6, 'disgusted': 5, 'fearful': 4, \n",
        "            'happy': 1, 'neutral': 0, 'sad': 2, 'surprised': 3\n",
        "        }\n",
        "        \n",
        "        self.samples = []\n",
        "        for emotion_folder in os.listdir(root_dir):\n",
        "            emotion_path = os.path.join(root_dir, emotion_folder)\n",
        "            if os.path.isdir(emotion_path) and emotion_folder in self.emotion_map:\n",
        "                label = self.emotion_map[emotion_folder]\n",
        "                for img_name in os.listdir(emotion_path):\n",
        "                    img_path = os.path.join(emotion_path, img_name)\n",
        "                    self.samples.append((img_path, label))\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples)} images from {root_dir}\")\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        \n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            image = np.zeros((48, 48, 3), dtype=np.uint8)\n",
        "        \n",
        "        if len(image.shape) == 2:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "        elif image.shape[2] == 1:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "        \n",
        "        image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        image = image.astype(np.float32)\n",
        "        image = image[..., ::-1].copy()\n",
        "        image[..., 0] -= 91.4953\n",
        "        image[..., 1] -= 103.8827\n",
        "        image[..., 2] -= 131.0912\n",
        "        \n",
        "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "print(\"FER2013Dataset class defined:\")\n",
        "print(\"  - Emotion mapping: angry→6, disgusted→5, fearful→4, happy→1, neutral→0, sad→2, surprised→3\")\n",
        "print(\"  - Resize: 224x224 with INTER_NEAREST\")\n",
        "print(\"  - RGB mean normalization: R-91.4953, G-103.8827, B-131.0912\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainAugmentation:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "        \n",
        "    def __call__(self, image):\n",
        "        if np.random.rand() < self.p:\n",
        "            image = cv2.flip(image, 1)\n",
        "        \n",
        "        if np.random.rand() < self.p:\n",
        "            angle = np.random.uniform(-15, 15)\n",
        "            h, w = image.shape[:2]\n",
        "            M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)\n",
        "            image = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
        "        \n",
        "        if np.random.rand() < self.p:\n",
        "            brightness = np.random.uniform(0.8, 1.2)\n",
        "            image = np.clip(image * brightness, 0, 255).astype(np.uint8)\n",
        "        \n",
        "        if np.random.rand() < self.p:\n",
        "            contrast = np.random.uniform(0.8, 1.2)\n",
        "            mean = image.mean()\n",
        "            image = np.clip((image - mean) * contrast + mean, 0, 255).astype(np.uint8)\n",
        "        \n",
        "        return image\n",
        "\n",
        "train_augmentation = TrainAugmentation(p=0.5)\n",
        "\n",
        "print(\"Data augmentation pipeline created:\")\n",
        "print(\"  - Horizontal flip (p=0.5)\")\n",
        "print(\"  - Random rotation ±15° (p=0.5)\")\n",
        "print(\"  - Brightness adjustment 0.8-1.2x (p=0.5)\")\n",
        "print(\"  - Contrast adjustment 0.8-1.2x (p=0.5)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "train_dataset = FER2013Dataset(TRAIN_PATH, transform=train_augmentation, is_training=True)\n",
        "test_dataset = FER2013Dataset(TEST_PATH, transform=None, is_training=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"\\nDataLoaders created:\")\n",
        "print(f\"  - Train batches: {len(train_loader)} (batch_size={BATCH_SIZE})\")\n",
        "print(f\"  - Test batches: {len(test_loader)} (batch_size={BATCH_SIZE})\")\n",
        "print(f\"  - Total train samples: {len(train_dataset)}\")\n",
        "print(f\"  - Total test samples: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ResNet50(num_classes=7, channels=3)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Model initialized: ResNet50\")\n",
        "print(f\"  - Input: 224x224x3 RGB images\")\n",
        "print(f\"  - Output: 7 emotion classes\")\n",
        "print(f\"  - Total parameters: {total_params:,}\")\n",
        "print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  - Device: {device}\")\n",
        "print(f\"  - Loss function: CrossEntropyLoss\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-7)\n",
        "\n",
        "print(f\"Optimizer configured:\")\n",
        "print(f\"  - Type: Adam\")\n",
        "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  - Weight decay: {WEIGHT_DECAY}\")\n",
        "print(f\"  - Scheduler: ReduceLROnPlateau\")\n",
        "print(f\"    * Factor: 0.5 (halves LR)\")\n",
        "print(f\"    * Patience: 3 epochs\")\n",
        "print(f\"    * Min LR: 1e-7\")\n",
        "print(f\"  - Training epochs: {NUM_EPOCHS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
        "    for images, labels in progress_bar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
        "    \n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(val_loader, desc='Validation', leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
        "    \n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "print(\"Training and validation functions defined\")\n",
        "print(\"  - train_epoch(): trains model for one epoch with progress tracking\")\n",
        "print(\"  - validate_epoch(): evaluates model on validation set\")\n",
        "print(\"  - Both return loss and accuracy metrics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = {\n",
        "    'train_loss': [], 'train_acc': [],\n",
        "    'val_loss': [], 'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_path = '/content/drive/MyDrive/Facial Analysis/models/FER_static_ResNet50_AffectNet.pt'\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"Starting Training - {NUM_EPOCHS} Epochs\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = validate_epoch(model, test_loader, criterion, device)\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
        "    \n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"✓ Best model saved! Val Acc: {val_acc:.2f}%\")\n",
        "    \n",
        "    scheduler.step(val_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Learning Rate: {current_lr:.2e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Training Complete!\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"Model saved to: {best_model_path}\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "epochs_range = range(1, NUM_EPOCHS + 1)\n",
        "\n",
        "axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(epochs_range, history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n",
        "axes[1].plot(epochs_range, history['val_acc'], 'r-', label='Val Accuracy', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/Facial Analysis/models/training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Training curves saved to Google Drive\")\n",
        "print(f\"Final Train Accuracy: {history['train_acc'][-1]:.2f}%\")\n",
        "print(f\"Best Validation Accuracy: {max(history['val_acc']):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"Evaluating best model on test set...\")\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc='Evaluating'):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "        \n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "emotion_names = ['Neutral', 'Happiness', 'Sadness', 'Surprise', 'Fear', 'Disgust', 'Anger']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(all_labels, all_preds, target_names=emotion_names, digits=4))\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "accuracy_per_class = cm.diagonal() / cm.sum(axis=1) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PER-CLASS ACCURACY\")\n",
        "print(\"=\" * 70)\n",
        "for i, emotion in enumerate(emotion_names):\n",
        "    print(f\"{emotion:12s}: {accuracy_per_class[i]:6.2f}%\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "overall_acc = 100. * sum(cm.diagonal()) / cm.sum()\n",
        "print(f\"\\nOverall Test Accuracy: {overall_acc:.2f}%\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_model = ResNet50(num_classes=7, channels=3)\n",
        "test_model.load_state_dict(torch.load(best_model_path))\n",
        "test_model.to(device)\n",
        "test_model.eval()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL COMPATIBILITY VERIFICATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "test_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "with torch.no_grad():\n",
        "    output = test_model(test_input)\n",
        "    features = test_model.extract_features(test_input)\n",
        "\n",
        "print(f\"✓ Model loaded successfully\")\n",
        "print(f\"✓ Input shape: {test_input.shape}\")\n",
        "print(f\"✓ Output shape: {output.shape} (batch, 7 classes)\")\n",
        "print(f\"✓ Feature shape: {features.shape} (batch, 512 features)\")\n",
        "print(f\"✓ Model architecture matches realtime_facial_analysis.py\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"USAGE INSTRUCTIONS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"1. Download the trained model from Google Drive:\")\n",
        "print(f\"   Location: /content/drive/MyDrive/Facial Analysis/models/FER_static_ResNet50_AffectNet.pt\")\n",
        "print()\n",
        "print(\"2. Place it in your project models/ directory:\")\n",
        "print(f\"   your-project/models/FER_static_ResNet50_AffectNet.pt\")\n",
        "print()\n",
        "print(\"3. Run realtime facial analysis:\")\n",
        "print(\"   python realtime_facial_analysis.py\")\n",
        "print()\n",
        "print(\"4. Model specifications:\")\n",
        "print(f\"   - Architecture: ResNet50 with custom Conv2dSame\")\n",
        "print(f\"   - Input: 224x224 RGB images\")\n",
        "print(f\"   - Preprocessing: BGR to RGB + mean normalization\")\n",
        "print(f\"   - Output: 7 emotions (Neutral, Happiness, Sadness, Surprise, Fear, Disgust, Anger)\")\n",
        "print(f\"   - Test Accuracy: {overall_acc:.2f}%\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 786787,
          "sourceId": 1351797,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 885385,
          "sourceId": 1504266,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 1028436,
          "sourceId": 1732825,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2577853,
          "sourceId": 4400131,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2740238,
          "sourceId": 4735388,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3160143,
          "sourceId": 5471526,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30458,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
